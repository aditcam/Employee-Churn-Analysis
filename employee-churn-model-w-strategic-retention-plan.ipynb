{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ad291e7e0207ec13d76b1f7ce52018fe68c81f35"
   },
   "source": [
    "# Employee Churn Model with a Strategic Retention Plan: a HR Analytics Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c5d91e1b1b141e164b2c15e3215b1ef75ead72d1",
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Problem-Definition\" data-toc-modified-id=\"Problem-Definition-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Problem Definition</a></span><ul class=\"toc-item\"><li><span><a href=\"#Project-Overview\" data-toc-modified-id=\"Project-Overview-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Project Overview</a></span></li><li><span><a href=\"#Problem-Statement\" data-toc-modified-id=\"Problem-Statement-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Problem Statement</a></span></li></ul></li><li><span><a href=\"#Dataset-Analysis\" data-toc-modified-id=\"Dataset-Analysis-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Dataset Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Importing-Python-libraries\" data-toc-modified-id=\"Importing-Python-libraries-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Importing Python libraries</a></span></li><li><span><a href=\"#Importing-the-data\" data-toc-modified-id=\"Importing-the-data-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Importing the data</a></span></li><li><span><a href=\"#Data-Description-and-Exploratory-Visualisations\" data-toc-modified-id=\"Data-Description-and-Exploratory-Visualisations-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Data Description and Exploratory Visualisations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Overview</a></span></li><li><span><a href=\"#Numerical-features-overview\" data-toc-modified-id=\"Numerical-features-overview-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Numerical features overview</a></span></li></ul></li><li><span><a href=\"#Feature-distribution-by-target-attribute\" data-toc-modified-id=\"Feature-distribution-by-target-attribute-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Feature distribution by target attribute</a></span><ul class=\"toc-item\"><li><span><a href=\"#Age\" data-toc-modified-id=\"Age-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>Age</a></span></li><li><span><a href=\"#Education\" data-toc-modified-id=\"Education-2.4.2\"><span class=\"toc-item-num\">2.4.2&nbsp;&nbsp;</span>Education</a></span></li><li><span><a href=\"#Gender\" data-toc-modified-id=\"Gender-2.4.3\"><span class=\"toc-item-num\">2.4.3&nbsp;&nbsp;</span>Gender</a></span></li><li><span><a href=\"#Marital-Status\" data-toc-modified-id=\"Marital-Status-2.4.4\"><span class=\"toc-item-num\">2.4.4&nbsp;&nbsp;</span>Marital Status</a></span></li><li><span><a href=\"#Distance-from-Home\" data-toc-modified-id=\"Distance-from-Home-2.4.5\"><span class=\"toc-item-num\">2.4.5&nbsp;&nbsp;</span>Distance from Home</a></span></li><li><span><a href=\"#Department\" data-toc-modified-id=\"Department-2.4.6\"><span class=\"toc-item-num\">2.4.6&nbsp;&nbsp;</span>Department</a></span></li><li><span><a href=\"#Role-and-Work-Conditions\" data-toc-modified-id=\"Role-and-Work-Conditions-2.4.7\"><span class=\"toc-item-num\">2.4.7&nbsp;&nbsp;</span>Role and Work Conditions</a></span></li><li><span><a href=\"#Years-at-the-Company\" data-toc-modified-id=\"Years-at-the-Company-2.4.8\"><span class=\"toc-item-num\">2.4.8&nbsp;&nbsp;</span>Years at the Company</a></span></li><li><span><a href=\"#Years-With-Current-Manager\" data-toc-modified-id=\"Years-With-Current-Manager-2.4.9\"><span class=\"toc-item-num\">2.4.9&nbsp;&nbsp;</span>Years With Current Manager</a></span></li><li><span><a href=\"#Work-Life-Balance-Score\" data-toc-modified-id=\"Work-Life-Balance-Score-2.4.10\"><span class=\"toc-item-num\">2.4.10&nbsp;&nbsp;</span>Work-Life Balance Score</a></span></li><li><span><a href=\"#Pay/Salary-Employee-Information\" data-toc-modified-id=\"Pay/Salary-Employee-Information-2.4.11\"><span class=\"toc-item-num\">2.4.11&nbsp;&nbsp;</span>Pay/Salary Employee Information</a></span></li><li><span><a href=\"#Employee-Satisfaction-and-Performance-Information\" data-toc-modified-id=\"Employee-Satisfaction-and-Performance-Information-2.4.12\"><span class=\"toc-item-num\">2.4.12&nbsp;&nbsp;</span>Employee Satisfaction and Performance Information</a></span></li></ul></li><li><span><a href=\"#Target-Variable:-Attrition\" data-toc-modified-id=\"Target-Variable:-Attrition-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Target Variable: Attrition</a></span></li><li><span><a href=\"#Correlation\" data-toc-modified-id=\"Correlation-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Correlation</a></span></li><li><span><a href=\"#EDA-Concluding-Remarks\" data-toc-modified-id=\"EDA-Concluding-Remarks-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>EDA Concluding Remarks</a></span></li></ul></li><li><span><a href=\"#Pre-processing-Pipeline\" data-toc-modified-id=\"Pre-processing-Pipeline-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Pre-processing Pipeline</a></span><ul class=\"toc-item\"><li><span><a href=\"#Encoding\" data-toc-modified-id=\"Encoding-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Encoding</a></span></li><li><span><a href=\"#Feature-Scaling\" data-toc-modified-id=\"Feature-Scaling-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Feature Scaling</a></span></li><li><span><a href=\"#Splitting-data-into-training-and-testing-sets\" data-toc-modified-id=\"Splitting-data-into-training-and-testing-sets-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Splitting data into training and testing sets</a></span></li></ul></li><li><span><a href=\"#Building-Machine-Learning-Models\" data-toc-modified-id=\"Building-Machine-Learning-Models-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Building Machine Learning Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Baseline-Algorithms\" data-toc-modified-id=\"Baseline-Algorithms-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Baseline Algorithms</a></span></li><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Logistic Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Fine-tuning\" data-toc-modified-id=\"Fine-tuning-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Fine-tuning</a></span></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Evaluation</a></span></li></ul></li><li><span><a href=\"#Random-Forest-Classifier\" data-toc-modified-id=\"Random-Forest-Classifier-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Random Forest Classifier</a></span><ul class=\"toc-item\"><li><span><a href=\"#Fine-tuning\" data-toc-modified-id=\"Fine-tuning-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>Fine-tuning</a></span></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-4.3.2\"><span class=\"toc-item-num\">4.3.2&nbsp;&nbsp;</span>Evaluation</a></span></li></ul></li><li><span><a href=\"#ROC-Graphs\" data-toc-modified-id=\"ROC-Graphs-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>ROC Graphs</a></span></li></ul></li><li><span><a href=\"#Concluding-Remarks\" data-toc-modified-id=\"Concluding-Remarks-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Concluding Remarks</a></span><ul class=\"toc-item\"><li><span><a href=\"#Risk-Category\" data-toc-modified-id=\"Risk-Category-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Risk Category</a></span></li><li><span><a href=\"#Strategic-Retention-Plan\" data-toc-modified-id=\"Strategic-Retention-Plan-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Strategic Retention Plan</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f5af9083834ec777823a1790ca6697faa7546de5"
   },
   "source": [
    "## Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "211137f297123106157e3b66f4ab07ac0ba6af1c"
   },
   "source": [
    "### Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "64561b3bd4a394fc112693ef1cb9b4a9932c7d7b"
   },
   "source": [
    "Employee turn-over (also known as \"employee churn\") is a costly problem for companies. The true cost of replacing an employee\n",
    "can often be quite large. A study by the [Center for American Progress](https://www.americanprogress.org/wp-content/uploads/2012/11/CostofTurnover.pdf) found that companies typically pay about one-fifth of an employeeâ€™s salary to replace that employee, and the cost can significantly increase if executives or highest-paid employees are to be replaced. In other words, the cost of replacing employees for most employers remains significant. This is due to the amount of time spent to interview and find a replacement, sign-on bonuses, and the loss of productivity for several months while the new employee gets accustomed to the new role. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ecb4a842621aa2ef8b35592e1afddab16f3da4ee"
   },
   "source": [
    "### Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "73ba878b54ca9029b800cf4add0761c8ee0fbe47"
   },
   "source": [
    "Understanding why and when employees are most likely to leave can lead to actions to improve employee retention as well as possibly planning new hiring in advance. I will be usign a step-by-step systematic approach using a method that could be used for a variety of ML problems. This project would fall under what is commonly known as \"**HR Anlytics**\", \"**People Analytics**\". <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "855b0f2c4b1637701b89c90bdfd37d633f1bdef4"
   },
   "source": [
    "In this study, we will attempt to solve the following problem statement is: <br>\n",
    "> ** What is the likelihood of an active employee leaving the company? <br>\n",
    "What are the key indicators of an employee leaving the company? <br>\n",
    "What policies or strategies can be adopted based on the results to improve employee retention? **\n",
    "\n",
    "Given that we have data on former employees, this is a standard **supervised classification problem** where the label is a binary variable, 0 (active employee), 1 (former employee). In this study, our target variable Y is the probability of an employee leaving the company. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "96c0abc6e014dfd295c28e9d313ebcab9603c3e4"
   },
   "source": [
    "![title](https://images.unsplash.com/photo-1523006520266-d3a4a8152803?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1422&q=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1219b77f4e057d44fef54ae60973c80f6b175cb8"
   },
   "source": [
    "## Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "37772f84746ec72fd06f0d42c715feee841bf006"
   },
   "source": [
    "In this case study, a HR dataset was sourced from [IBM HR Analytics Employee Attrition & Performance](https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/) which contains employee data for 1,470 employees with various information about the employees. I will use this dataset to predict when employees are going to quit by understanding the main drivers of employee churn. <br>\n",
    "\n",
    "As stated on the [IBM website](https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/) *\"This is a fictional data set created by IBM data scientists\"*. Its main purpose was to demonstrate the IBM Watson Analytics tool for employee attrition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1575cdb84ecf04680cb1b2b5532d5e1a2f526503"
   },
   "source": [
    "### Importing Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:03:57.588868Z",
     "start_time": "2019-02-26T17:03:55.460102Z"
    },
    "_uuid": "ba67030f5bfbdcc3e5d2be30b668735b8e1ce194"
   },
   "outputs": [],
   "source": [
    "# importing libraries for data handling and analysis\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "from openpyxl import load_workbook\n",
    "import numpy as np\n",
    "from scipy.stats import norm, skew\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:03:58.509004Z",
     "start_time": "2019-02-26T17:03:57.735796Z"
    },
    "_uuid": "eb5999b0dec8aaa63acd5f4b26d83ce8deab7060",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# importing libraries for data visualisations\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "color = sns.color_palette()\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = None\n",
    "# Standard plotly imports\n",
    "from plotly import plotly as py\n",
    "import plotly as py\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "#py.initnotebookmode(connected=True) # this code, allow us to work with offline plotly version\n",
    "# Using plotly + cufflinks in offline mode\n",
    "import cufflinks as cf\n",
    "cf.set_config_file(offline=True)\n",
    "import cufflinks\n",
    "cufflinks.go_offline(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:04:04.775128Z",
     "start_time": "2019-02-26T17:04:03.078922Z"
    },
    "_uuid": "bf95a75f42fa7a6a628609e097d2b3f24ad59fb0"
   },
   "outputs": [],
   "source": [
    "# sklearn modules for preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "# from imblearn.over_sampling import SMOTE  # SMOTE\n",
    "# sklearn modules for ML model selection\n",
    "from sklearn.model_selection import train_test_split  # import 'train_test_split'\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Libraries for data modelling\n",
    "from sklearn import svm, tree, linear_model, neighbors\n",
    "from sklearn import naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Common sklearn Model Helpers\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "# from sklearn.datasets import make_classification\n",
    "\n",
    "# sklearn modules for performance metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve\n",
    "from sklearn.metrics import auc, roc_auc_score, roc_curve, recall_score, log_loss\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, make_scorer\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:03:58.615595Z",
     "start_time": "2019-02-26T17:03:58.611072Z"
    },
    "_uuid": "50831020556791dfd20070201f1726489d57f708"
   },
   "outputs": [],
   "source": [
    "# importing misceallenous libraries\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import timeit\n",
    "import string\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "from dateutil.parser import parse\n",
    "# ip = get_ipython()\n",
    "# ip.register_magics(jupyternotify.JupyterNotifyMagics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1721e003da2eec3ce67f4a7214369549ec81271c"
   },
   "source": [
    "### Importing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6eb57109b61e071793678eaa8631d49e8ff8f7f9"
   },
   "source": [
    "> Let's import the dataset and make of a copy of the source file for this analysis. <br> The dataset contains 1,470 rows and 35 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1e6b3456d78b56441980c94f5aa8974b705bc423"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:04:13.722057Z",
     "start_time": "2019-02-26T17:04:13.190727Z"
    },
    "_uuid": "df75a50115f9153f1b65b317973e833404b92e02"
   },
   "outputs": [],
   "source": [
    "# Read Excel file\n",
    "df_sourcefile = pd.read_csv('../input/WA_Fn-UseC_-HR-Employee-Attrition.csv')\n",
    "print(\"Shape of dataframe is: {}\".format(df_sourcefile.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:04:14.574804Z",
     "start_time": "2019-02-26T17:04:14.56641Z"
    },
    "_uuid": "af8ec6049db492a40b20c40cd94fcdfa83d1a4f5"
   },
   "outputs": [],
   "source": [
    "# Make a copy of the original sourcefile\n",
    "df_HR = df_sourcefile.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3314f787ee48983121d950bcfd1ff9ce0b7b2bab"
   },
   "source": [
    "### Data Description and Exploratory Visualisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0511008c21df5ec748d096bd77b97ab8ebee98b3"
   },
   "source": [
    "> In this section, we will provide data visualizations that summarizes or extracts relevant characteristics of features in our dataset. Let's look at each column in detail, get a better understanding of the dataset, and group them together when appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "eeefc0cd321d8e47114cbbe06b3c80485dc2ff24"
   },
   "source": [
    "#### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:04:15.992407Z",
     "start_time": "2019-02-26T17:04:15.984336Z"
    },
    "_uuid": "e6f8f499a51b18ea60a0f1968d7e2ef905e7d974"
   },
   "outputs": [],
   "source": [
    "# Dataset columns\n",
    "df_HR.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:04:17.560239Z",
     "start_time": "2019-02-26T17:04:17.496713Z"
    },
    "_uuid": "f7a185921a4ae04cde7eed45860a9b8281f35be7"
   },
   "outputs": [],
   "source": [
    "# Dataset header\n",
    "df_HR.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "04ff59a6215ca610d49d0615cedc2183aab3d7cf"
   },
   "source": [
    "> The dataset contains several numerical and categorical columns providing various information on employee's personal and employment details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:04:28.699004Z",
     "start_time": "2019-02-26T17:04:28.678744Z"
    },
    "_uuid": "6597c6293d9f0d42c5144ba82a798525b37ea369"
   },
   "outputs": [],
   "source": [
    "# let's break down the columns by their type (i.e. int64, float64, object)\n",
    "df_HR.columns.to_series().groupby(df_HR.dtypes).groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:04:31.771322Z",
     "start_time": "2019-02-26T17:04:31.760624Z"
    },
    "_uuid": "31abfaf023a4674813d44fd234761da618683fa8"
   },
   "outputs": [],
   "source": [
    "# Columns datatypes and missign values\n",
    "df_HR.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d9cbfb12f213d8a4e1079707010f6d7528a439e9"
   },
   "source": [
    "> The data provided has no missing values. In HR Analytics, employee data is unlikely to feature large ratio of missing values as HR Departments typically have all personal and employment data on-file. However, the type of documentation data is being kept in (i.e. whether it is paper-based, Excel spreadhsheets, databases, etc) has a massive impact on the accuracy and the ease of access to the HR data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "670dd0f340ddbe728526a7c5a9ab6c7ef2be7568"
   },
   "source": [
    "#### Numerical features overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:07:46.256407Z",
     "start_time": "2019-02-26T17:07:46.157177Z"
    },
    "_uuid": "5993b107aad273bb52deef2f451ed81112235883"
   },
   "outputs": [],
   "source": [
    "df_HR.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:10:32.790863Z",
     "start_time": "2019-02-26T17:10:30.177055Z"
    },
    "_uuid": "18ed4bc99e4c0a476b19babd9514d0df65d3d580"
   },
   "outputs": [],
   "source": [
    "df_HR.hist(figsize=(20,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2bbf3dd29eddf84b02344a3e32c4dcea1d14dc4a"
   },
   "source": [
    "> A few observations can be made based on the information and histograms for numerical features:\n",
    " - Many histograms are tail-heavy; indeed several distributions are right-skewed (e.g. MonthlyIncome DistanceFromHome, YearsAtCompany). Data transformation methods may be required to approach a normal distribution prior to fitting a model to the data.\n",
    " - Age distribution is a slightly right-skewed normal distribution with the bulk of the staff between 25 and 45 years old.\n",
    " - EmployeeCount and StandardHours are constant values for all employees. They're likely to be redundant features.\n",
    " - Employee Number is likely to be a unique identifier for employees given the feature's quasi-uniform distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "aa7705d1b70ca5343826913f549cf91f9f5a07f2"
   },
   "source": [
    "### Feature distribution by target attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8a1b718557cb00e1c33c8a337d9998e9d433187f"
   },
   "source": [
    "#### Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "92fdfea8c3991c8fa93a2b8813b81abe3ed5288e"
   },
   "source": [
    "> The age distributions for Active and Ex-employees only differs by one year. <br>\n",
    "The average age of ex-employees is **33.6** years old, while **37.6** is the average age for current employees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:17:11.519877Z",
     "start_time": "2019-02-26T17:17:11.505111Z"
    },
    "_uuid": "131677fb0a094901c9349c9de1a1403cd1ed824d"
   },
   "outputs": [],
   "source": [
    "(mu, sigma) = norm.fit(df_HR.loc[df_HR['Attrition'] == 'Yes', 'Age'])\n",
    "print(\n",
    "    'Ex-exmployees: average age = {:.1f} years old and standard deviation = {:.1f}'.format(mu, sigma))\n",
    "(mu, sigma) = norm.fit(df_HR.loc[df_HR['Attrition'] == 'No', 'Age'])\n",
    "print('Current exmployees: average age = {:.1f} years old and standard deviation = {:.1f}'.format(\n",
    "    mu, sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ffb401426173f9e12f8687905918bd66a7ad6900"
   },
   "source": [
    "> Let's create a kernel density estimation (KDE) plot colored by the value of the target. A kernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable. It will allow us to identify if there is a correlation between the Age of the Client and their ability to pay it back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "619da2efb8e4ec8cb04a287d9d6e8db23b472e94"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.style.use('seaborn-colorblind')\n",
    "plt.grid(True, alpha=0.5)\n",
    "sns.kdeplot(df_HR.loc[df_HR['Attrition'] == 'No', 'Age'], label = 'Active Employee')\n",
    "sns.kdeplot(df_HR.loc[df_HR['Attrition'] == 'Yes', 'Age'], label = 'Ex-Employees')\n",
    "plt.xlim(left=18, right=60)\n",
    "plt.xlabel('Age (years)')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Age Distribution in Percent by Attrition Status');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "34ae1478661293e8c5c896b4bcf6a3c4ccf24014"
   },
   "source": [
    "#### Education"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d6ba8500fcca007b51dceceed46d4f7a02e2b0ea"
   },
   "source": [
    "> Several Education Fields are represented in the dataset, namely: Human Resources, Life Sciences, Marketing, Medical, Technical Degree, and a miscellaneous category Other. Here, I plot the normalized % of Leavers for each Education Field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c05d6547508a573779a3ef539332e3f9e8ffa5bb"
   },
   "outputs": [],
   "source": [
    "# Education Field of employees\n",
    "df_HR['EducationField'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "487e5bf5b57e872a1b6a065f515bdb69efb70f7b"
   },
   "outputs": [],
   "source": [
    "df_EducationField = pd.DataFrame(columns=[\"Field\", \"% of Leavers\"])\n",
    "i=0\n",
    "for field in list(df_HR['EducationField'].unique()):\n",
    "    ratio = df_HR[(df_HR['EducationField']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] / df_HR[df_HR['EducationField']==field].shape[0]\n",
    "    df_EducationField.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \n",
    "df_EF = df_EducationField.groupby(by=\"Field\").sum()\n",
    "df_EF.iplot(kind='bar',title='Leavers by Education Field (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2964b5b93209a4bb3afd00002691b2339d100812"
   },
   "source": [
    "#### Gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1d98229ec19d639c0618c8e1509df540154cf496"
   },
   "source": [
    "> Gender distribution shows that the dataset features a higher relative proportion of male ex-employees than female ex-employees, with normalised gender distribution of ex-employees in the dataset at 17.0% for Males and 14.8% for Females."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4354b67d12eb40cc70efae71fb66334e771f4c9b"
   },
   "outputs": [],
   "source": [
    "# Gender of employees\n",
    "df_HR['Gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:24:03.445736Z",
     "start_time": "2019-02-26T17:24:03.43055Z"
    },
    "_uuid": "79050e93315bbaad9bf396cec32ef4b572194ec4"
   },
   "outputs": [],
   "source": [
    "print(\"Normalised gender distribution of ex-employees in the dataset: Male = {:.1f}%; Female {:.1f}%.\".format((df_HR[(df_HR['Attrition'] == 'Yes') & (\n",
    "    df_HR['Gender'] == 'Male')].shape[0] / df_HR[df_HR['Gender'] == 'Male'].shape[0])*100, (df_HR[(df_HR['Attrition'] == 'Yes') & (df_HR['Gender'] == 'Female')].shape[0] / df_HR[df_HR['Gender'] == 'Female'].shape[0])*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "45cbbd3f28f1d38ce74288c085e73b0d3529b24a"
   },
   "outputs": [],
   "source": [
    "df_Gender = pd.DataFrame(columns=[\"Gender\", \"% of Leavers\"])\n",
    "i=0\n",
    "for field in list(df_HR['Gender'].unique()):\n",
    "    ratio = df_HR[(df_HR['Gender']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] / df_HR[df_HR['Gender']==field].shape[0]\n",
    "    df_Gender.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \n",
    "df_G = df_Gender.groupby(by=\"Gender\").sum()\n",
    "df_G.iplot(kind='bar',title='Leavers by Gender (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b1ed4bc0685b732b5b8d9e3c86fa3695d57f5d3a"
   },
   "source": [
    "#### Marital Status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "15ff35493a99b494b41029e1f8aeef759e07c93f"
   },
   "source": [
    "> The dataset features three marital status: Married (673 employees), Single (470 employees), Divorced (327 employees). <br>\n",
    "Single employees show the largest proportion of leavers at 25%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:24:23.536407Z",
     "start_time": "2019-02-26T17:24:23.527184Z"
    },
    "_uuid": "b3add32de040d84dc5d0cb2bd14176d3a320e002"
   },
   "outputs": [],
   "source": [
    "# Marital Status of employees\n",
    "df_HR['MaritalStatus'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c7a57a439f28177092d1740cc4d6fe656ca0a290"
   },
   "outputs": [],
   "source": [
    "df_Marital = pd.DataFrame(columns=[\"Marital Status\", \"% of Leavers\"])\n",
    "i=0\n",
    "for field in list(df_HR['MaritalStatus'].unique()):\n",
    "    ratio = df_HR[(df_HR['MaritalStatus']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] / df_HR[df_HR['MaritalStatus']==field].shape[0]\n",
    "    df_Marital.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \n",
    "df_MF = df_Marital.groupby(by=\"Marital Status\").sum()\n",
    "df_MF.iplot(kind='bar',title='Leavers by Marital Status (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7b7da86c3e381416cdff9f7695143185207ca5f7"
   },
   "source": [
    "#### Distance from Home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3cc3e6df7fd132eaa669768eac3f89fb00eccce5"
   },
   "source": [
    "> Distance from home for employees to get to work varies from 1 to 29 miles. There is no discernable strong correlation between Distance from Home and Attrition Status as per the KDE plot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:24:39.197221Z",
     "start_time": "2019-02-26T17:24:39.179336Z"
    },
    "_uuid": "bece3344740a4031e5714fb45fb0416afaec6254"
   },
   "outputs": [],
   "source": [
    "# Distance from Home\n",
    "print(\"Distance from home for employees to get to work is from {} to {} miles.\".format(df_HR['DistanceFromHome'].min(),\n",
    "                                                                                       df_HR['DistanceFromHome'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:24:39.868875Z",
     "start_time": "2019-02-26T17:24:39.857155Z"
    },
    "_uuid": "ba5dbbd103e2b2e4670da5a3044c1cda34863d0e"
   },
   "outputs": [],
   "source": [
    "print('Average distance from home for currently active employees: {:.2f} miles and ex-employees: {:.2f} miles'.format(\n",
    "    df_HR[df_HR['Attrition'] == 'No']['DistanceFromHome'].mean(), df_HR[df_HR['Attrition'] == 'Yes']['DistanceFromHome'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "61649e54c7e1e524cf838c182429cea6b1509f91"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.style.use('seaborn-colorblind')\n",
    "plt.grid(True, alpha=0.5)\n",
    "sns.kdeplot(df_HR.loc[df_HR['Attrition'] == 'No', 'DistanceFromHome'], label = 'Active Employee')\n",
    "sns.kdeplot(df_HR.loc[df_HR['Attrition'] == 'Yes', 'DistanceFromHome'], label = 'Ex-Employees')\n",
    "plt.xlabel('DistanceFromHome')\n",
    "plt.xlim(left=0)\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distance From Home Distribution in Percent by Attrition Status');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "317689f80bba2a2c42d5458b043cb6170af14abd"
   },
   "source": [
    "#### Department"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e58a938e86d65b507ae9bbb0fab4677e78139deb"
   },
   "source": [
    "> The data features employee data from three departments: Research & Development, Sales, and Human Resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:25:51.988039Z",
     "start_time": "2019-02-26T17:25:51.976578Z"
    },
    "_uuid": "a4a6a348baad1ff0c9dc31cd37d3b34e8ca53e09"
   },
   "outputs": [],
   "source": [
    "# The organisation consists of several departments\n",
    "df_HR['Department'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7e9c50a554007204bbbdac8536be984e5a304235"
   },
   "outputs": [],
   "source": [
    "df_Department = pd.DataFrame(columns=[\"Department\", \"% of Leavers\"])\n",
    "i=0\n",
    "for field in list(df_HR['Department'].unique()):\n",
    "    ratio = df_HR[(df_HR['Department']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] / df_HR[df_HR['Department']==field].shape[0]\n",
    "    df_Department.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \n",
    "df_DF = df_Department.groupby(by=\"Department\").sum()\n",
    "df_DF.iplot(kind='bar',title='Leavers by Department (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "545f2b58943c1cd4cc2d34e4804b3b0d97624bb6"
   },
   "source": [
    "#### Role and Work Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "299b857f3a7ee43b49cabffa30209667d307a4f8"
   },
   "source": [
    "> A preliminary look at the relationship between Business Travel frequency and Attrition Status shows that there is a largest normalized proportion of Leavers for employees that travel \"frequently\". Travel metrics associated with Business Travel status were not disclosed (i.e. how many hours of Travel is considered \"Frequent\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:26:02.75959Z",
     "start_time": "2019-02-26T17:26:02.751269Z"
    },
    "_uuid": "210d81b6b1d3f2ad76d72331add6ad3b15ce1b9d"
   },
   "outputs": [],
   "source": [
    "# Employees have different business travel commitmnent depending on their roles and level in the organisation\n",
    "df_HR['BusinessTravel'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ae677588bfeef1586c67d2e76bbd283c8ba3514f"
   },
   "outputs": [],
   "source": [
    "df_BusinessTravel = pd.DataFrame(columns=[\"Business Travel\", \"% of Leavers\"])\n",
    "i=0\n",
    "for field in list(df_HR['BusinessTravel'].unique()):\n",
    "    ratio = df_HR[(df_HR['BusinessTravel']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] / df_HR[df_HR['BusinessTravel']==field].shape[0]\n",
    "    df_BusinessTravel.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \n",
    "df_BT = df_BusinessTravel.groupby(by=\"Business Travel\").sum()\n",
    "df_BT.iplot(kind='bar',title='Leavers by Business Travel (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e0ea21d51cdffc8d71db25eae9f11699e6f64dd2"
   },
   "source": [
    "> Several Job Roles are listed in the dataset: Sales Executive, Research Scientist, Laboratory Technician, Manufacturing Director, Healthcare Representative, Manager, Sales Representative, Research Director, Human Resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:32:43.178962Z",
     "start_time": "2019-02-26T17:32:43.171236Z"
    },
    "_uuid": "b31fed2d44b6af7dd17be0bc087924f8f4b27355"
   },
   "outputs": [],
   "source": [
    "# Employees in the database have several roles on-file\n",
    "df_HR['JobRole'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e341712dae1bd5cec1f76ed1d6820e426f72099c"
   },
   "outputs": [],
   "source": [
    "df_JobRole = pd.DataFrame(columns=[\"Job Role\", \"% of Leavers\"])\n",
    "i=0\n",
    "for field in list(df_HR['JobRole'].unique()):\n",
    "    ratio = df_HR[(df_HR['JobRole']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] / df_HR[df_HR['JobRole']==field].shape[0]\n",
    "    df_JobRole.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \n",
    "df_JR = df_JobRole.groupby(by=\"Job Role\").sum()\n",
    "df_JR.iplot(kind='bar',title='Leavers by Job Role (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3716078ce114ffc0d2fe4d11fc97ce78883115b9"
   },
   "source": [
    "> Employees have an assigned level within the organisation which varies from 1 (staff) to 5 (managerial/director). Employees with an assigned Job Level of \"1\" show the largest normalized proportion of Leavers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:32:53.123041Z",
     "start_time": "2019-02-26T17:32:53.112061Z"
    },
    "_uuid": "1c60295cb3be15a3dae39096e226a1f432fa6f21"
   },
   "outputs": [],
   "source": [
    "df_HR['JobLevel'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f33d17dfaa6266a282d7c6c1bdcf8ac9e005d1e5"
   },
   "outputs": [],
   "source": [
    "df_JobLevel = pd.DataFrame(columns=[\"Job Level\", \"% of Leavers\"])\n",
    "i=0\n",
    "for field in list(df_HR['JobLevel'].unique()):\n",
    "    ratio = df_HR[(df_HR['JobLevel']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] / df_HR[df_HR['JobLevel']==field].shape[0]\n",
    "    df_JobLevel.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \n",
    "df_JL = df_JobLevel.groupby(by=\"Job Level\").sum()\n",
    "df_JL.iplot(kind='bar',title='Leavers by Job Level (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1f8be35ac51c0211958473eb622fe187e77cbee9"
   },
   "source": [
    "> A ranking is associated to the employee's Job Involvement :1 'Low' 2 'Medium' 3 'High' 4 'Very High'. The plot below indicates a negative correlation with the Job Involvement of an employee and the Attrition Status. In other words, employees with higher Job Involvement are less likely to leave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:32:55.526061Z",
     "start_time": "2019-02-26T17:32:55.516931Z"
    },
    "_uuid": "2e74831233e3889cd5544c7febe63302d20ae354"
   },
   "outputs": [],
   "source": [
    "df_HR['JobInvolvement'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9ea774c2f7fdf6a15dd84dc4bf038f40fe41b6b6"
   },
   "outputs": [],
   "source": [
    "df_JobInvolvement = pd.DataFrame(columns=[\"Job Involvement\", \"% of Leavers\"])\n",
    "i=0\n",
    "for field in list(df_HR['JobInvolvement'].unique()):\n",
    "    ratio = df_HR[(df_HR['JobInvolvement']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] / df_HR[df_HR['JobInvolvement']==field].shape[0]\n",
    "    df_JobInvolvement.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \n",
    "df_JI = df_JobInvolvement.groupby(by=\"Job Involvement\").sum()\n",
    "df_JI.iplot(kind='bar',title='Leavers by Job Involvement (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1318c7c6bb9252aaa255d3755429bb7126272e5a"
   },
   "source": [
    "> The data indicates that employees may have access to some Training. A feature indicates how many years it's been since the employee attended such training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:32:57.284487Z",
     "start_time": "2019-02-26T17:32:57.276298Z"
    },
    "_uuid": "ede58ed74cca87849af5085b01b9c62ce1d79d6e"
   },
   "outputs": [],
   "source": [
    "print(\"Number of training times last year varies from {} to {} years.\".format(\n",
    "    df_HR['TrainingTimesLastYear'].min(), df_HR['TrainingTimesLastYear'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "11b75d5d6e558a705b6b6fd82db470e87c0cb20f"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.style.use('seaborn-colorblind')\n",
    "plt.grid(True, alpha=0.5)\n",
    "sns.kdeplot(df_HR.loc[df_HR['Attrition'] == 'No', 'TrainingTimesLastYear'], label = 'Active Employee')\n",
    "sns.kdeplot(df_HR.loc[df_HR['Attrition'] == 'Yes', 'TrainingTimesLastYear'], label = 'Ex-Employees')\n",
    "plt.xlabel('TrainingTimesLastYear')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Training Times Last Year Distribution in Percent by Attrition Status');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cc2f0b7e23dd565e1726a72dd20941de2db4f8b6"
   },
   "source": [
    "> There is a feature for the number of companies the employee has worked at. <br>\n",
    "> 0 likely indicates that according to records, the employee has only worked at this company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:32:58.858512Z",
     "start_time": "2019-02-26T17:32:58.848698Z"
    },
    "_uuid": "3d1c29486dbe6fb89ce6ccc26a2bbe9336660f89"
   },
   "outputs": [],
   "source": [
    "df_HR['NumCompaniesWorked'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3691d2500758b3573b3e6bbc51ccedbf75a9ec17"
   },
   "outputs": [],
   "source": [
    "df_NumCompaniesWorked = pd.DataFrame(columns=[\"Num Companies Worked\", \"% of Leavers\"])\n",
    "i=0\n",
    "for field in list(df_HR['NumCompaniesWorked'].unique()):\n",
    "    ratio = df_HR[(df_HR['NumCompaniesWorked']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] / df_HR[df_HR['NumCompaniesWorked']==field].shape[0]\n",
    "    df_NumCompaniesWorked.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \n",
    "df_NC = df_NumCompaniesWorked.groupby(by=\"Num Companies Worked\").sum()\n",
    "df_NC.iplot(kind='bar',title='Leavers by Num Companies Worked (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a25dae7657c4cc98e662e5690d342ad9bde9190e"
   },
   "source": [
    "#### Years at the Company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:33:04.945211Z",
     "start_time": "2019-02-26T17:33:04.939946Z"
    },
    "_uuid": "e9f770d1e42aa5ad3ae6d2d9a3d8fb9d53784213"
   },
   "outputs": [],
   "source": [
    "print(\"Number of Years at the company varies from {} to {} years.\".format(\n",
    "    df_HR['YearsAtCompany'].min(), df_HR['YearsAtCompany'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5a8923667f7dcd170d82080858c44a6142fffcc0"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.style.use('seaborn-colorblind')\n",
    "plt.grid(True, alpha=0.5)\n",
    "sns.kdeplot(df_HR.loc[df_HR['Attrition'] == 'No', 'YearsAtCompany'], label = 'Active Employee')\n",
    "sns.kdeplot(df_HR.loc[df_HR['Attrition'] == 'Yes', 'YearsAtCompany'], label = 'Ex-Employees')\n",
    "plt.xlabel('YearsAtCompany')\n",
    "plt.xlim(left=0)\n",
    "plt.ylabel('Density')\n",
    "plt.title('Years At Company in Percent by Attrition Status');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:33:19.903172Z",
     "start_time": "2019-02-26T17:33:19.898332Z"
    },
    "_uuid": "74e281cc2013d7cad6066861faf88543b2e626e2"
   },
   "outputs": [],
   "source": [
    "print(\"Number of Years in the current role varies from {} to {} years.\".format(\n",
    "    df_HR['YearsInCurrentRole'].min(), df_HR['YearsInCurrentRole'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d7aeef0a3c689584a818c1db3932b43a303e3109"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.style.use('seaborn-colorblind')\n",
    "plt.grid(True, alpha=0.5)\n",
    "sns.kdeplot(df_HR.loc[df_HR['Attrition'] == 'No', 'YearsInCurrentRole'], label = 'Active Employee')\n",
    "sns.kdeplot(df_HR.loc[df_HR['Attrition'] == 'Yes', 'YearsInCurrentRole'], label = 'Ex-Employees')\n",
    "plt.xlabel('YearsInCurrentRole')\n",
    "plt.xlim(left=0)\n",
    "plt.ylabel('Density')\n",
    "plt.title('Years In Current Role in Percent by Attrition Status');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:33:20.652097Z",
     "start_time": "2019-02-26T17:33:20.645465Z"
    },
    "_uuid": "c959f7018c502a55f3dd5a8467ce6ecbac96a7b6"
   },
   "outputs": [],
   "source": [
    "print(\"Number of Years since last promotion varies from {} to {} years.\".format(\n",
    "    df_HR['YearsSinceLastPromotion'].min(), df_HR['YearsSinceLastPromotion'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6919fb3ef0fd3edaec7d94c7be37c536bb0c5aec"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.style.use('seaborn-colorblind')\n",
    "plt.grid(True, alpha=0.5)\n",
    "sns.kdeplot(df_HR.loc[df_HR['Attrition'] == 'No', 'YearsSinceLastPromotion'], label = 'Active Employee')\n",
    "sns.kdeplot(df_HR.loc[df_HR['Attrition'] == 'Yes', 'YearsSinceLastPromotion'], label = 'Ex-Employees')\n",
    "plt.xlabel('YearsSinceLastPromotion')\n",
    "plt.xlim(left=0)\n",
    "plt.ylabel('Density')\n",
    "plt.title('Years Since Last Promotion in Percent by Attrition Status');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:33:24.595137Z",
     "start_time": "2019-02-26T17:33:24.58703Z"
    },
    "_uuid": "10d85a7db33d93c551b8456a36bc0931f558d689"
   },
   "outputs": [],
   "source": [
    "print(\"Total working years varies from {} to {} years.\".format(\n",
    "    df_HR['TotalWorkingYears'].min(), df_HR['TotalWorkingYears'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "02ee2f8f6de6a48dabd6146dbdfc1920bc57ebe8"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.style.use('seaborn-colorblind')\n",
    "plt.grid(True, alpha=0.5)\n",
    "sns.kdeplot(df_HR.loc[df_HR['Attrition'] == 'No', 'TotalWorkingYears'], label = 'Active Employee')\n",
    "sns.kdeplot(df_HR.loc[df_HR['Attrition'] == 'Yes', 'TotalWorkingYears'], label = 'Ex-Employees')\n",
    "plt.xlabel('TotalWorkingYears')\n",
    "plt.xlim(left=0)\n",
    "plt.ylabel('Density')\n",
    "plt.title('Total Working Years in Percent by Attrition Status');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f73a6842f7b27214fffafa7a0597b333248afc07"
   },
   "source": [
    "#### Years With Current Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:33:28.589679Z",
     "start_time": "2019-02-26T17:33:28.582117Z"
    },
    "_uuid": "f91c7ec5fea5bcc60f50b73b6c2b443fb8001d31"
   },
   "outputs": [],
   "source": [
    "print(\"Number of Years wit current manager varies from {} to {} years.\".format(\n",
    "    df_HR['YearsWithCurrManager'].min(), df_HR['YearsWithCurrManager'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3a85b89b3edf75d7d628eee4004b4b03f82933c9"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.style.use('seaborn-colorblind')\n",
    "plt.grid(True, alpha=0.5)\n",
    "sns.kdeplot(df_HR.loc[df_HR['Attrition'] == 'No', 'YearsWithCurrManager'], label = 'Active Employee')\n",
    "sns.kdeplot(df_HR.loc[df_HR['Attrition'] == 'Yes', 'YearsWithCurrManager'], label = 'Ex-Employees')\n",
    "plt.xlabel('YearsWithCurrManager')\n",
    "plt.xlim(left=0)\n",
    "plt.ylabel('Density')\n",
    "plt.title('Years With Curr Manager in Percent by Attrition Status');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "361ddc9f6badda77ed9ea20ea684e312c54367b0"
   },
   "source": [
    "#### Work-Life Balance Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b9cbe242ad6eee0da0eb2c60d44ff32a75478f09"
   },
   "source": [
    "> A feature related to \"Work-Life Balance\" was captured as: 1 'Bad' 2 'Good' 3 'Better' 4 'Best'. The data indicates that the largest normalised proportion of Leavers had \"Bad\" Work-Life Balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:33:37.907622Z",
     "start_time": "2019-02-26T17:33:37.896498Z"
    },
    "_uuid": "baded8e599c55229086e827d9a1312c46fbec984"
   },
   "outputs": [],
   "source": [
    "df_HR['WorkLifeBalance'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:33:40.167381Z",
     "start_time": "2019-02-26T17:33:38.117086Z"
    },
    "_uuid": "dca967c20af497a9387f9b02e3fa7850daf48380"
   },
   "outputs": [],
   "source": [
    "df_WorkLifeBalance = pd.DataFrame(columns=[\"WorkLifeBalance\", \"% of Leavers\"])\n",
    "i=0\n",
    "for field in list(df_HR['WorkLifeBalance'].unique()):\n",
    "    ratio = df_HR[(df_HR['WorkLifeBalance']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] / df_HR[df_HR['WorkLifeBalance']==field].shape[0]\n",
    "    df_WorkLifeBalance.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \n",
    "df_WLB = df_WorkLifeBalance.groupby(by=\"WorkLifeBalance\").sum()\n",
    "df_WLB.iplot(kind='bar',title='Leavers by WorkLifeBalance (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c94f2adf5b1052eb981f999fe26b811ef86366be"
   },
   "source": [
    "> All employees have a standard 80-hour work commitment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:33:42.180449Z",
     "start_time": "2019-02-26T17:33:42.167857Z"
    },
    "_uuid": "52950a258c452124223d912a0c86b1f831ade604"
   },
   "outputs": [],
   "source": [
    "df_HR['StandardHours'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "de5cba0963bbb6b19fbd2ea9f79ccfb8f846e00c"
   },
   "source": [
    "> Some employees have overtime commitments. The data clearly show that there is significant larger portion of employees with OT that have left the company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:33:24.61319Z",
     "start_time": "2019-02-26T17:33:24.598613Z"
    },
    "_uuid": "7e8bbf9878e56897d261d948b5f543c602d23cf3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_HR['OverTime'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fa46aa3e231fdaf672aff59b3c34c933df822e61"
   },
   "outputs": [],
   "source": [
    "df_OverTime = pd.DataFrame(columns=[\"OverTime\", \"% of Leavers\"])\n",
    "i=0\n",
    "for field in list(df_HR['OverTime'].unique()):\n",
    "    ratio = df_HR[(df_HR['OverTime']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] / df_HR[df_HR['OverTime']==field].shape[0]\n",
    "    df_OverTime.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \n",
    "df_OT = df_OverTime.groupby(by=\"OverTime\").sum()\n",
    "df_OT.iplot(kind='bar',title='Leavers by OverTime (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6e9d65c6816251e3a8b6b44d1d740176d00be4e2"
   },
   "source": [
    "#### Pay/Salary Employee Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:33:45.871154Z",
     "start_time": "2019-02-26T17:33:45.863994Z"
    },
    "_uuid": "c6a890b5e0557b3664996218bd9779f3c346d354"
   },
   "outputs": [],
   "source": [
    "print(\"Employee Hourly Rate varies from ${} to ${}.\".format(\n",
    "    df_HR['HourlyRate'].min(), df_HR['HourlyRate'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:33:47.521697Z",
     "start_time": "2019-02-26T17:33:47.513959Z"
    },
    "_uuid": "0a84f35d1fc4b194966c5c46a2079ffd75523a7f"
   },
   "outputs": [],
   "source": [
    "print(\"Employee Daily Rate varies from ${} to ${}.\".format(\n",
    "    df_HR['DailyRate'].min(), df_HR['DailyRate'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:33:49.163333Z",
     "start_time": "2019-02-26T17:33:49.155766Z"
    },
    "_uuid": "fcba15d3f7529f8fe175d04c5df08ca28ea37197"
   },
   "outputs": [],
   "source": [
    "print(\"Employee Monthly Rate varies from ${} to ${}.\".format(\n",
    "    df_HR['MonthlyRate'].min(), df_HR['MonthlyRate'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:33:50.559036Z",
     "start_time": "2019-02-26T17:33:50.551761Z"
    },
    "_uuid": "c1aa81d6931de8b051e42399c9a811f83c27301f"
   },
   "outputs": [],
   "source": [
    "print(\"Employee Monthly Income varies from ${} to ${}.\".format(\n",
    "    df_HR['MonthlyIncome'].min(), df_HR['MonthlyIncome'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "877e4cf6b1601b016f96f88b899822f6cdd684b0"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.style.use('seaborn-colorblind')\n",
    "plt.grid(True, alpha=0.5)\n",
    "sns.kdeplot(df_HR.loc[df_HR['Attrition'] == 'No', 'MonthlyIncome'], label = 'Active Employee')\n",
    "sns.kdeplot(df_HR.loc[df_HR['Attrition'] == 'Yes', 'MonthlyIncome'], label = 'Ex-Employees')\n",
    "plt.xlabel('Monthly Income')\n",
    "plt.xlim(left=0)\n",
    "plt.ylabel('Density')\n",
    "plt.title('Monthly Income in Percent by Attrition Status');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:33:57.173763Z",
     "start_time": "2019-02-26T17:33:57.160269Z"
    },
    "_uuid": "9f9540c18a1ee80f2ddf8d7b706dc26d27e2d254"
   },
   "outputs": [],
   "source": [
    "print(\"Percentage Salary Hikes varies from {}% to {}%.\".format(\n",
    "    df_HR['PercentSalaryHike'].min(), df_HR['PercentSalaryHike'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "986ac842f1d5f8b476a0e7528f1a2b64a6a6bece"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.style.use('seaborn-colorblind')\n",
    "plt.grid(True, alpha=0.5)\n",
    "sns.kdeplot(df_HR.loc[df_HR['Attrition'] == 'No', 'PercentSalaryHike'], label = 'Active Employee')\n",
    "sns.kdeplot(df_HR.loc[df_HR['Attrition'] == 'Yes', 'PercentSalaryHike'], label = 'Ex-Employees')\n",
    "plt.xlabel('PercentSalaryHike')\n",
    "plt.xlim(left=0)\n",
    "plt.ylabel('Density')\n",
    "plt.title('Percent Salary Hike in Percent by Attrition Status');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:34:01.300618Z",
     "start_time": "2019-02-26T17:34:01.288154Z"
    },
    "_uuid": "389432ad135ddf88ead52cb8b226dbbe1605999c"
   },
   "outputs": [],
   "source": [
    "print(\"Stock Option Levels varies from {} to {}.\".format(\n",
    "    df_HR['StockOptionLevel'].min(), df_HR['StockOptionLevel'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:34:05.306907Z",
     "start_time": "2019-02-26T17:34:05.281163Z"
    },
    "_uuid": "7c9030663cec74935f268d60076e8a423f9f900a"
   },
   "outputs": [],
   "source": [
    "print(\"Normalised percentage of leavers by Stock Option Level: 1: {:.2f}%, 2: {:.2f}%, 3: {:.2f}%\".format(\n",
    "    df_HR[(df_HR['Attrition'] == 'Yes') & (df_HR['StockOptionLevel'] == 1)\n",
    "          ].shape[0] / df_HR[df_HR['StockOptionLevel'] == 1].shape[0]*100,\n",
    "    df_HR[(df_HR['Attrition'] == 'Yes') & (df_HR['StockOptionLevel'] == 2)\n",
    "          ].shape[0] / df_HR[df_HR['StockOptionLevel'] == 1].shape[0]*100,\n",
    "    df_HR[(df_HR['Attrition'] == 'Yes') & (df_HR['StockOptionLevel'] == 3)].shape[0] / df_HR[df_HR['StockOptionLevel'] == 1].shape[0]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:34:07.3257Z",
     "start_time": "2019-02-26T17:34:05.492995Z"
    },
    "_uuid": "8d1d08282b74771ae3c0e74276c25183873a16d3"
   },
   "outputs": [],
   "source": [
    "df_StockOptionLevel = pd.DataFrame(columns=[\"StockOptionLevel\", \"% of Leavers\"])\n",
    "i=0\n",
    "for field in list(df_HR['StockOptionLevel'].unique()):\n",
    "    ratio = df_HR[(df_HR['StockOptionLevel']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] / df_HR[df_HR['StockOptionLevel']==field].shape[0]\n",
    "    df_StockOptionLevel.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \n",
    "df_SOL = df_StockOptionLevel.groupby(by=\"StockOptionLevel\").sum()\n",
    "df_SOL.iplot(kind='bar',title='Leavers by Stock Option Level (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3e9d4770b0f8a8360301369095a9735fc4056803"
   },
   "source": [
    "#### Employee Satisfaction and Performance Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "46aded3f7af6a805ab87d0a6b339cdc201b794bd"
   },
   "source": [
    "> Environment Satisfaction was captured as: 1 'Low' 2 'Medium' 3 'High' 4 'Very High'. <br> \n",
    "Proportion of Leaving Employees decreases as the Environment Satisfaction score increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:34:10.643374Z",
     "start_time": "2019-02-26T17:34:10.633995Z"
    },
    "_uuid": "72321c718bf4d6212e017dd948b9f11b4aea0e75"
   },
   "outputs": [],
   "source": [
    "df_HR['EnvironmentSatisfaction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1ac1ccd6208bd86e0b75ab60c8d4ccaf7a9f66c9"
   },
   "outputs": [],
   "source": [
    "df_EnvironmentSatisfaction = pd.DataFrame(columns=[\"EnvironmentSatisfaction\", \"% of Leavers\"])\n",
    "i=0\n",
    "for field in list(df_HR['EnvironmentSatisfaction'].unique()):\n",
    "    ratio = df_HR[(df_HR['EnvironmentSatisfaction']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] / df_HR[df_HR['EnvironmentSatisfaction']==field].shape[0]\n",
    "    df_EnvironmentSatisfaction.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \n",
    "df_Env = df_EnvironmentSatisfaction.groupby(by=\"EnvironmentSatisfaction\").sum()\n",
    "df_Env.iplot(kind='bar',title='Leavers by Environment Satisfaction (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ecdab47f57cef76f0f429b36729e4b7891d95fe5"
   },
   "source": [
    "> Job Satisfaction was captured as: 1 'Low' 2 'Medium' 3 'High' 4 'Very High'. <br> \n",
    "Proportion of Leaving Employees decreases as the Job Satisfaction score increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:34:13.476654Z",
     "start_time": "2019-02-26T17:34:13.462306Z"
    },
    "_uuid": "3545ac027797c354b9d0f2e6b7ccefbb3db4ea60"
   },
   "outputs": [],
   "source": [
    "# Job Satisfaction was captured as: 1 'Low' 2 'Medium' 3 'High' 4 'Very High'\n",
    "df_HR['JobSatisfaction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:34:20.063756Z",
     "start_time": "2019-02-26T17:34:18.049645Z"
    },
    "_uuid": "46d94168714b1661ca9c64b5f9da8637fde3ee16"
   },
   "outputs": [],
   "source": [
    "df_JobSatisfaction = pd.DataFrame(columns=[\"JobSatisfaction\", \"% of Leavers\"])\n",
    "i=0\n",
    "for field in list(df_HR['JobSatisfaction'].unique()):\n",
    "    ratio = df_HR[(df_HR['JobSatisfaction']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] / df_HR[df_HR['JobSatisfaction']==field].shape[0]\n",
    "    df_JobSatisfaction.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \n",
    "df_JS = df_JobSatisfaction.groupby(by=\"JobSatisfaction\").sum()\n",
    "df_JS.iplot(kind='bar',title='Leavers by Job Satisfaction (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ac5dd4cc5c9174478383e57e0d9d67eafb7d984a"
   },
   "source": [
    "> Relationship Satisfaction was captured as: 1 'Low', 2 'Medium', 3 'High', 4 'Very High'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:34:21.624859Z",
     "start_time": "2019-02-26T17:34:21.615539Z"
    },
    "_uuid": "6e4e8d004d3810c8b6305bdbfa2d81bab888feb8"
   },
   "outputs": [],
   "source": [
    "df_HR['RelationshipSatisfaction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c02c80ebe75dcf69754757238656d8260698d6ba"
   },
   "outputs": [],
   "source": [
    "df_RelationshipSatisfaction = pd.DataFrame(columns=[\"RelationshipSatisfaction\", \"% of Leavers\"])\n",
    "i=0\n",
    "for field in list(df_HR['RelationshipSatisfaction'].unique()):\n",
    "    ratio = df_HR[(df_HR['RelationshipSatisfaction']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] / df_HR[df_HR['RelationshipSatisfaction']==field].shape[0]\n",
    "    df_RelationshipSatisfaction.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \n",
    "df_RS = df_RelationshipSatisfaction.groupby(by=\"RelationshipSatisfaction\").sum()\n",
    "df_RS.iplot(kind='bar',title='Leavers by Relationship Satisfaction (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a3fd3b0d7b4fb357322f0e89f251e638eb99ed5f"
   },
   "source": [
    "> Employee Performance Rating was captured as: 1 'Low' 2 'Good' 3 'Excellent' 4 'Outstanding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:34:23.293321Z",
     "start_time": "2019-02-26T17:34:23.284867Z"
    },
    "_uuid": "8f585593670912ee0205aade1d625df9e3ce45a1"
   },
   "outputs": [],
   "source": [
    "df_HR['PerformanceRating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:34:23.314235Z",
     "start_time": "2019-02-26T17:34:23.296543Z"
    },
    "_uuid": "6d60361e7c84bb5914bfffb101a090d8f64df3f3"
   },
   "outputs": [],
   "source": [
    "print(\"Normalised percentage of leavers by Stock Option Level: 3: {:.2f}%, 4: {:.2f}%\".format(\n",
    "    df_HR[(df_HR['Attrition'] == 'Yes') & (df_HR['PerformanceRating'] == 3)\n",
    "          ].shape[0] / df_HR[df_HR['StockOptionLevel'] == 1].shape[0]*100,\n",
    "    df_HR[(df_HR['Attrition'] == 'Yes') & (df_HR['PerformanceRating'] == 4)].shape[0] / df_HR[df_HR['StockOptionLevel'] == 1].shape[0]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:34:26.576253Z",
     "start_time": "2019-02-26T17:34:24.766264Z"
    },
    "_uuid": "aece704a9db4104ea8a218e40aff75b6b7f1fa23"
   },
   "outputs": [],
   "source": [
    "df_PerformanceRating = pd.DataFrame(columns=[\"PerformanceRating\", \"% of Leavers\"])\n",
    "i=0\n",
    "for field in list(df_HR['PerformanceRating'].unique()):\n",
    "    ratio = df_HR[(df_HR['PerformanceRating']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] / df_HR[df_HR['PerformanceRating']==field].shape[0]\n",
    "    df_PerformanceRating.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \n",
    "df_PR = df_PerformanceRating.groupby(by=\"PerformanceRating\").sum()\n",
    "df_PR.iplot(kind='bar',title='Leavers by Performance Rating (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cbce1fad9b7a33c85cf1c2e7f0c7f79395c893e5"
   },
   "source": [
    "### Target Variable: Attrition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e2aa72b3a79703e66bd71d44a006eef8c8bdfffd"
   },
   "source": [
    "> The feature 'Attrition' is what this Machine Learning problem is about. We are trying to predict the value of the feature 'Attrition' by using other related features associated with the employee's personal and professional history. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:34:30.057344Z",
     "start_time": "2019-02-26T17:34:30.044363Z"
    },
    "_uuid": "7a8a6a074c3eb13a0eed4851bc527e660a3a1355"
   },
   "outputs": [],
   "source": [
    "# Attrition indicates if the employee is currently active ('No') or has left the company ('Yes')\n",
    "df_HR['Attrition'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:34:31.567472Z",
     "start_time": "2019-02-26T17:34:31.556605Z"
    },
    "_uuid": "ea36db9641a9c54b1f6343e42132c9e3b8c7481f"
   },
   "outputs": [],
   "source": [
    "print(\"Percentage of Current Employees is {:.1f}% and of Ex-employees is: {:.1f}%\".format(\n",
    "    df_HR[df_HR['Attrition'] == 'No'].shape[0] / df_HR.shape[0]*100,\n",
    "    df_HR[df_HR['Attrition'] == 'Yes'].shape[0] / df_HR.shape[0]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:34:33.217588Z",
     "start_time": "2019-02-26T17:34:33.076203Z"
    },
    "_uuid": "eb1714364cd259b53048a06b2a19237f783428ce"
   },
   "outputs": [],
   "source": [
    "df_HR['Attrition'].iplot(kind='hist', xTitle='Attrition',\n",
    "                         yTitle='count', title='Attrition Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "316f74f16983987575b2ba90bb9f98ac8f6d4112"
   },
   "source": [
    "> As shown on the chart above, we see this is an imbalanced class problem. Indeed, the percentage of Current Employees in our dataset is 83.9% and the percentage of Ex-employees is: 16.1%\n",
    "\n",
    "> Machine learning algorithms typically work best when the number of instances of each classes are roughly equal. We will have to address this target feature imbalance prior to implementing our Machine Learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5b4e956c29fe317ae68b669da43f6c1252d21d8c"
   },
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3477a2bec6026410752335afa64cb277b07bb507"
   },
   "source": [
    "> Let's take a look at some of most significant correlations. It is worth remembering that correlation coefficients only measure linear correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:43:13.26136Z",
     "start_time": "2019-02-26T17:43:13.224637Z"
    },
    "_uuid": "1e93d952f7255e783250e678038e65ab0cc1a1f6"
   },
   "outputs": [],
   "source": [
    "# Find correlations with the target and sort\n",
    "df_HR_trans = df_HR.copy()\n",
    "df_HR_trans['Target'] = df_HR_trans['Attrition'].apply(\n",
    "    lambda x: 0 if x == 'No' else 1)\n",
    "df_HR_trans = df_HR_trans.drop(\n",
    "    ['Attrition', 'EmployeeCount', 'EmployeeNumber', 'StandardHours', 'Over18'], axis=1)\n",
    "correlations = df_HR_trans.corr()['Target'].sort_values()\n",
    "print('Most Positive Correlations: \\n', correlations.tail(5))\n",
    "print('\\nMost Negative Correlations: \\n', correlations.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2d0724b7e8ac57890c0a447134d2a406fa3b65a0"
   },
   "source": [
    "> Let's plot a heatmap to visualize the correlation between Attrition and these factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:44:13.037172Z",
     "start_time": "2019-02-26T17:44:12.365082Z"
    },
    "_uuid": "398153d0bf2f8900d863df78256c488ce2f4bbf8"
   },
   "outputs": [],
   "source": [
    "# Calculate correlations\n",
    "corr = df_HR_trans.corr()\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "# Heatmap\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(corr,\n",
    "            vmax=.5,\n",
    "            mask=mask,\n",
    "            # annot=True, fmt='.2f',\n",
    "            linewidths=.2, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "03161964da62c28fc7c8fca334fcc3298674192a"
   },
   "source": [
    "> As shown above, \"Monthly Rate\", \"Number of Companies Worked\" and \"Distance From Home\" are positively correlated to Attrition; <br> while \"Total Working Years\", \"Job Level\", and \"Years In Current Role\" are negatively correlated to Attrition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "db7e2a1b23638d0aabce0061ed12635e436c6a6a"
   },
   "source": [
    "### EDA Concluding Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e79bceefc747595c9ad15a17c5dfaeb4dd2c617c"
   },
   "source": [
    "Let's summarise the findings from this EDA: <br>\n",
    "\n",
    "> - The dataset does not feature any missing or erroneous data values, and all features are of the correct data type. <br>\n",
    "- The strongest positive correlations with the target features are: **Performance Rating**, **Monthly Rate**, **Num Companies Worked**, **Distance From Home**. \n",
    "- The strongest negative correlations with the target features are: **Total Working Years**, **Job Level**, **Years In Current Role**, and **Monthly Income**.\n",
    "- The dataset is **imbalanced** with the majoriy of observations describing Currently Active Employees. <br>\n",
    "- Several features (ie columns) are redundant for our analysis, namely: EmployeeCount, EmployeeNumber, StandardHours, and Over18. <br>\n",
    "\n",
    "Other observations include: <br>\n",
    "> - Single employees show the largest proportion of leavers, compared to Married and Divorced counterparts. <br>\n",
    "- About 10% of leavers left when they reach their 2-year anniversary at the company. <br>\n",
    "- Loyal employees with higher salaries and more responsbilities show lower proportion of leavers compared to their counterparts. <br>\n",
    "- People who live further away from their work show higher proportion of leavers compared to their counterparts.<br>\n",
    "- People who travel frequently show higher proportion of leavers compared to their counterparts.<br>\n",
    "- People who have to work overtime show higher proportion of leavers compared to their counterparts.<br>\n",
    "- Employee who work as Sales Representatives show a significant percentage of Leavers in the submitted dataset.<br>\n",
    "- Employees that have already worked at several companies previously (already \"bounced\" between workplaces) show higher proportion of leavers compared to their counterparts.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f87028a5db34435c8a22f75b635aeb6f7351ec0e"
   },
   "source": [
    "![title](https://images.unsplash.com/photo-1498409785966-ab341407de6e?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1360&q=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ad6c688216630d2c031c3db5c864f521ee6b9014"
   },
   "source": [
    "## Pre-processing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d1014862d69ebee647197801c3940ce322fb6c6a"
   },
   "source": [
    "In this section, we undertake data pre-processing steps to prepare the datasets for Machine Learning algorithm implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9127684644f01ecbfb833d1d9e39245f56e5f7d9"
   },
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b9e044fb5d54e59664168e89b1e87a671c06367d"
   },
   "source": [
    "> Machine Learning algorithms can typically only have numerical values as their predictor variables. Hence Label Encoding becomes necessary as they encode categorical labels with numerical values. To avoid introducing feature importance for categorical features with large numbers of unique values, we will use both Lable Encoding and One-Hot Encoding as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a8441da6c11ee0c94f0bdc78fafb388a2c54bb88"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "# Create a label encoder object\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5d2be09e6cc4afca61c6083f1b52dcb051a3041a"
   },
   "outputs": [],
   "source": [
    "print(df_HR.shape)\n",
    "df_HR.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cc7da6ed8096a3626a44318e10cf0f4fdb0aaea8"
   },
   "outputs": [],
   "source": [
    "# Label Encoding will be used for columns with 2 or less unique values\n",
    "le_count = 0\n",
    "for col in df_HR.columns[1:]:\n",
    "    if df_HR[col].dtype == 'object':\n",
    "        if len(list(df_HR[col].unique())) <= 2:\n",
    "            le.fit(df_HR[col])\n",
    "            df_HR[col] = le.transform(df_HR[col])\n",
    "            le_count += 1\n",
    "print('{} columns were label encoded.'.format(le_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "76a039f2176c4affba1def82cc0265ba69fd4118"
   },
   "outputs": [],
   "source": [
    "# convert rest of categorical variable into dummy\n",
    "df_HR = pd.get_dummies(df_HR, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c87cc0e3d3d9b94db14f9d7bc4fefff577be9065"
   },
   "source": [
    "> The resulting dataframe has **49 columns** for 1,470 employees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "90bdb3c0a21f3ddb07273bbc9c51d831436383f2"
   },
   "outputs": [],
   "source": [
    "print(df_HR.shape)\n",
    "df_HR.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "01574a5d6ad26cd6c1ac81027d151be3fd774f58"
   },
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ff183a52206911ee2aff0524f9693538dbd3b643"
   },
   "source": [
    "> Feature Scaling using MinMaxScaler essentially shrinks the range such that the range is now between 0 and n. Machine Learning algorithms perform better when input numerical variables fall within a similar scale. In this case, we are scaling between 0 and 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "576adeb1310411f9e3c624fab1ca453ed0a945b5"
   },
   "outputs": [],
   "source": [
    "# import MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 5))\n",
    "HR_col = list(df_HR.columns)\n",
    "HR_col.remove('Attrition')\n",
    "for col in HR_col:\n",
    "    df_HR[col] = df_HR[col].astype(float)\n",
    "    df_HR[[col]] = scaler.fit_transform(df_HR[[col]])\n",
    "df_HR['Attrition'] = pd.to_numeric(df_HR['Attrition'], downcast='float')\n",
    "df_HR.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "41aaef830d7d113df61325d2425f1c5c040eabdf"
   },
   "outputs": [],
   "source": [
    "print('Size of Full Encoded Dataset: {}'. format(df_HR.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "39fe1b2a0b514e4dd2fde658e20e0d5f21a5cf28"
   },
   "source": [
    "### Splitting data into training and testing sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ab0f98414ae605f8fc5450356cc50f1399b538cd"
   },
   "source": [
    "> Prior to implementating or applying any Machine Learning algorithms, we must decouple training and testing datasets from our master dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fcc7ed15165fc9e3c0a7eb3a9b6e303ac30375f2"
   },
   "outputs": [],
   "source": [
    "# assign the target to a new dataframe and convert it to a numerical feature\n",
    "#df_target = df_HR[['Attrition']].copy()\n",
    "target = df_HR['Attrition'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "75de11eb0f1c901c2a8cfaa9a97502d616f8018b"
   },
   "outputs": [],
   "source": [
    "type(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "31347bd0917cbd8b6ee12a11135ac084f0f128ce",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's remove the target feature and redundant features from the dataset\n",
    "df_HR.drop(['Attrition', 'EmployeeCount', 'EmployeeNumber',\n",
    "            'StandardHours', 'Over18'], axis=1, inplace=True)\n",
    "print('Size of Full dataset is: {}'.format(df_HR.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "60f4f60af5a5978322324e87f414e60c75e8ae00"
   },
   "outputs": [],
   "source": [
    "# Since we have class imbalance (i.e. more employees with turnover=0 than turnover=1)\n",
    "# let's use stratify=y to maintain the same ratio as in the training dataset when splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_HR,\n",
    "                                                    target,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=7,\n",
    "                                                    stratify=target)  \n",
    "print(\"Number transactions X_train dataset: \", X_train.shape)\n",
    "print(\"Number transactions y_train dataset: \", y_train.shape)\n",
    "print(\"Number transactions X_test dataset: \", X_test.shape)\n",
    "print(\"Number transactions y_test dataset: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5ded907c93c9417165c23a18373a8d798f6fd308"
   },
   "source": [
    "## Building Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "06c69dd3b2ea10d1f5d21c559e63e732bdd061a1"
   },
   "source": [
    "### Baseline Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8ea0617f53a466a2137ec7d225791bd161f43bb2"
   },
   "source": [
    "> Let's first use a range of **baseline** algorithms (using out-of-the-box hyper-parameters) before we move on to more sophisticated solutions. The algorithms considered in this section are: **Logistic Regression**, **Random Forest**, **SVM**, **KNN**, **Decision Tree Classifier**, **Gaussian NB**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "34f0f077c817a6ce11c5f3808a1cdbcfd36d4e4b"
   },
   "outputs": [],
   "source": [
    "# selection of algorithms to consider and set performance measure\n",
    "models = []\n",
    "models.append(('Logistic Regression', LogisticRegression(solver='liblinear', random_state=7,\n",
    "                                                         class_weight='balanced')))\n",
    "models.append(('Random Forest', RandomForestClassifier(\n",
    "    n_estimators=100, random_state=7)))\n",
    "models.append(('SVM', SVC(gamma='auto', random_state=7)))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('Decision Tree Classifier',\n",
    "               DecisionTreeClassifier(random_state=7)))\n",
    "models.append(('Gaussian NB', GaussianNB()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "984c4b6b7abfc5d1d4425de3667edace55d61bb2"
   },
   "source": [
    "> Let's evaluate each model in turn and provide accuracy and standard deviation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "776a831f1c2a416950cc7a8e38992b021ad9a180"
   },
   "outputs": [],
   "source": [
    "acc_results = []\n",
    "auc_results = []\n",
    "names = []\n",
    "# set table to table to populate with performance results\n",
    "col = ['Algorithm', 'ROC AUC Mean', 'ROC AUC STD', \n",
    "       'Accuracy Mean', 'Accuracy STD']\n",
    "df_results = pd.DataFrame(columns=col)\n",
    "i = 0\n",
    "# evaluate each model using cross-validation\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(\n",
    "        n_splits=10, random_state=7)  # 10-fold cross-validation\n",
    "\n",
    "    cv_acc_results = model_selection.cross_val_score(  # accuracy scoring\n",
    "        model, X_train, y_train, cv=kfold, scoring='accuracy')\n",
    "\n",
    "    cv_auc_results = model_selection.cross_val_score(  # roc_auc scoring\n",
    "        model, X_train, y_train, cv=kfold, scoring='roc_auc')\n",
    "\n",
    "    acc_results.append(cv_acc_results)\n",
    "    auc_results.append(cv_auc_results)\n",
    "    names.append(name)\n",
    "    df_results.loc[i] = [name,\n",
    "                         round(cv_auc_results.mean()*100, 2),\n",
    "                         round(cv_auc_results.std()*100, 2),\n",
    "                         round(cv_acc_results.mean()*100, 2),\n",
    "                         round(cv_acc_results.std()*100, 2)\n",
    "                         ]\n",
    "    i += 1\n",
    "df_results.sort_values(by=['ROC AUC Mean'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2224271872f4a78c4ab94fd098c3edbd36883ca3"
   },
   "source": [
    "> **Classification Accuracy** is the number of correct predictions made as a ratio of all predictions made. <br> \n",
    "It is the most common evaluation metric for classification problems. However, it is often **misused** as it is only really suitable when there are an **equal number of observations in each class** and all predictions and prediction errors are equally important. It is not the case in this project, so a different scoring metric may be more suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "758615e925045d0216b32f0a0bbd079cd837df21"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 7))\n",
    "fig.suptitle('Algorithm Accuracy Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(acc_results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7adffc801414fffee04189d7fd466d6c11be7335"
   },
   "source": [
    "> **Area under ROC Curve** (or AUC for short) is a performance metric for binary classification problems. <br>\n",
    "The AUC represents a **modelâ€™s ability to discriminate between positive and negative classes**. An area of 1.0 represents a model that made all predictions perfectly. An area of 0.5 represents a model as good as random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5daf4bcf1407ca2951d70ecee5c10bbb34d0130a"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 7))\n",
    "fig.suptitle('Algorithm ROC AUC Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(auc_results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1dd2e95e967dd848c6c475c245aabcd029e9d056"
   },
   "source": [
    "> Based on our ROC AUC comparison analysis, **Logistic Regression** and **Random Forest** show the highest mean AUC scores. We will shortlist these two algorithms for further analysis. See below for more details on these two algos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "47b2f002c472d12f03e2c3342451b647c50e2c4c"
   },
   "source": [
    "**Logistic Regression** is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable. Logistic Regression is classification algorithm that is not as sophisticated as the ensemble methods or boosted decision trees method discussed below. Hence, it provides us with a good benchmark. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7f6cd6101264020002dca39f3f0a7048d09c309a"
   },
   "source": [
    "![title](https://cdn-images-1.medium.com/max/1600/0*vRhSdZ_k4wrP6Bl8.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d60e566795f9fbd57de095b30ccb46f9e3979629"
   },
   "source": [
    "**Random Forest** is a popular and versatile machine learning method that is capable of solving both regression and classification. Random Forest is a brand of Ensemble learning, as it relies on an ensemble of decision trees. It aggregates Classification (or Regression) Trees. A decision tree is composed of a series of decisions that can be used to classify an observation in a dataset.\n",
    "\n",
    "Random Forest fits a number of decision tree classifiers on various **sub-samples of the dataset** and use **averaging** to improve the predictive accuracy and control over-fitting. Random Forest can handle a large number of features, and is helpful for estimating which of your variables are important in the underlying data being modeled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "63ff623c9c867773945e2ec3e9a289dfc9c09a9b"
   },
   "source": [
    "![title](https://images.unsplash.com/photo-1441422454217-519d3ee81350?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1489&q=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cb2c28c0a83796d9f1775756a8529271d3c18273"
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f0fcab6db8074e65b07d4155a883da447a880a2b"
   },
   "source": [
    "> Let's take a closer look at using the Logistic Regression algorithm. I'll be using 10 fold Cross-Validation to train our Logistic Regression Model and estimate its AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ac33a2e108c417dc4615aff6a4627bb7799c0660"
   },
   "outputs": [],
   "source": [
    "kfold = model_selection.KFold(n_splits=10, random_state=7)\n",
    "modelCV = LogisticRegression(solver='liblinear',\n",
    "                             class_weight=\"balanced\", \n",
    "                             random_state=7)\n",
    "scoring = 'roc_auc'\n",
    "results = model_selection.cross_val_score(\n",
    "    modelCV, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "print(\"AUC score (STD): %.2f (%.2f)\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b13d489afaf6de9dc7f185465dd5e5d4e439e117"
   },
   "source": [
    "#### Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d8418f4d6b3dd0a12b35ce9a3acc2980f96922b9"
   },
   "source": [
    "> GridSearchCV allows use to fine-tune hyper-parameters by searching over specified parameter values for an estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f47bd3c0a8725bee802cc4e63eb55459bca9a8df"
   },
   "outputs": [],
   "source": [
    "param_grid = {'C': np.arange(1e-03, 2, 0.01)} # hyper-parameter list to fine-tune\n",
    "log_gs = GridSearchCV(LogisticRegression(solver='liblinear', # setting GridSearchCV\n",
    "                                         class_weight=\"balanced\", \n",
    "                                         random_state=7),\n",
    "                      iid=True,\n",
    "                      return_train_score=True,\n",
    "                      param_grid=param_grid,\n",
    "                      scoring='roc_auc',\n",
    "                      cv=10)\n",
    "\n",
    "log_grid = log_gs.fit(X_train, y_train)\n",
    "log_opt = log_grid.best_estimator_\n",
    "results = log_gs.cv_results_\n",
    "\n",
    "print('='*20)\n",
    "print(\"best params: \" + str(log_gs.best_estimator_))\n",
    "print(\"best params: \" + str(log_gs.best_params_))\n",
    "print('best score:', log_gs.best_score_)\n",
    "print('='*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5b69141a929df7f8fac939db3b82aa7af817eb9d"
   },
   "source": [
    "> As shown above, the results from GridSearchCV provided us with fine-tuned hyper-parameter using ROC_AUC as the scoring metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a26c7f836df7fbee0acb20e7cdd3f4e6fa70f441"
   },
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1e9ff129533ceb525912feb28c77ef13acf38c46"
   },
   "outputs": [],
   "source": [
    "## Confusion Matrix\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, log_opt.predict(X_test))\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "27d49b53d7b3d9dc4ca5fbece593aade3a2ffa81"
   },
   "outputs": [],
   "source": [
    "print('Accuracy of Logistic Regression Classifier on test set: {:.2f}'.format(log_opt.score(X_test, y_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2e6878ecce530861b3c31b409a9098c17504e93f"
   },
   "source": [
    "> The Confusion matrix provides us with a much more detailed representation of the accuracy score and of what's going on with our labels - we know exactly which/how labels were correctly and incorrectly predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8206de1a3abaa9c8af9b132a3c3499bd13b81824"
   },
   "outputs": [],
   "source": [
    "# Classification report for the optimised Log Regression\n",
    "log_opt.fit(X_train, y_train)\n",
    "print(classification_report(y_test, log_opt.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "981d6f73167457a6cd5d6a20f87ec0e9b907d0dc"
   },
   "source": [
    "> Instead of getting binary estimated target features (0 or 1), a probability can be associated with the predicted target. <br> The output provides a first index referring to the probability that the data belong to **class 0** (employee not leaving), and the second refers to the probability that the data belong to **class 1** (employee leaving).\n",
    "\n",
    "> The resulting AUC score is higher than that best score during the optimisation step. Predicting probabilities of a particular label provides us with a measure of how likely an employee is to leave the company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "31db150211376c49bfc7883a1ef6ad14bb3bb5d9"
   },
   "outputs": [],
   "source": [
    "log_opt.fit(X_train, y_train) # fit optimised model to the training data\n",
    "probs = log_opt.predict_proba(X_test) # predict probabilities\n",
    "probs = probs[:, 1] # we will only keep probabilities associated with the employee leaving\n",
    "logit_roc_auc = roc_auc_score(y_test, probs) # calculate AUC score using test dataset\n",
    "print('AUC score: %.3f' % logit_roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3482160a4c6d39004acbf738753bb8731417346e"
   },
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bcaba54a2a7f17d47eba4e832e6f51d281748bfb"
   },
   "source": [
    "> Let's take a closer look at using the Random Forest algorithm. I'll fine-tune the Random Forest algorithm's hyper-parameters by cross-validation against the AUC score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b6c9ac1f4bef140004998541fea17b51ff28dc98"
   },
   "source": [
    "#### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3a9eb6f541920a993fd077aa31b4dd607a3bcf08"
   },
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(class_weight = \"balanced\",\n",
    "                                       random_state=7)\n",
    "param_grid = {'n_estimators': [50, 75, 100, 125, 150, 175],\n",
    "              'min_samples_split':[2,4,6,8,10],\n",
    "              'min_samples_leaf': [1, 2, 3, 4],\n",
    "              'max_depth': [5, 10, 15, 20, 25]}\n",
    "\n",
    "grid_obj = GridSearchCV(rf_classifier,\n",
    "                        iid=True,\n",
    "                        return_train_score=True,\n",
    "                        param_grid=param_grid,\n",
    "                        scoring='roc_auc',\n",
    "                        cv=10)\n",
    "\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "rf_opt = grid_fit.best_estimator_\n",
    "\n",
    "print('='*20)\n",
    "print(\"best params: \" + str(grid_obj.best_estimator_))\n",
    "print(\"best params: \" + str(grid_obj.best_params_))\n",
    "print('best score:', grid_obj.best_score_)\n",
    "print('='*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "394b7c5b7ac73bb39dd9ecac25965bcdce56e9f3"
   },
   "source": [
    "> Random Forest allows us to know which features are of the most importance in predicting the target feature (\"attrition\" in this project). Below, we plot features by their importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a7368307dae8dd8189f2b9daf3d34de725ede944"
   },
   "outputs": [],
   "source": [
    "importances = rf_opt.feature_importances_\n",
    "indices = np.argsort(importances)[::-1] # Sort feature importances in descending order\n",
    "names = [X_train.columns[i] for i in indices] # Rearrange feature names so they match the sorted feature importances\n",
    "plt.figure(figsize=(15, 7)) # Create plot\n",
    "plt.title(\"Feature Importance\") # Create plot title\n",
    "plt.bar(range(X_train.shape[1]), importances[indices]) # Add bars\n",
    "plt.xticks(range(X_train.shape[1]), names, rotation=90) # Add feature names as x-axis labels\n",
    "plt.show() # Show plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2db3e91d57f558e7365bd1ca93160bdd29379297"
   },
   "source": [
    "> Random Forest helped us identify the Top 10 most important indicators (ranked in the table below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6c2bc9610283e71874f28e6b4526dbbea6f92eed"
   },
   "outputs": [],
   "source": [
    "importances = rf_opt.feature_importances_\n",
    "df_param_coeff = pd.DataFrame(columns=['Feature', 'Coefficient'])\n",
    "for i in range(44):\n",
    "    feat = X_train.columns[i]\n",
    "    coeff = importances[i]\n",
    "    df_param_coeff.loc[i] = (feat, coeff)\n",
    "df_param_coeff.sort_values(by='Coefficient', ascending=False, inplace=True)\n",
    "df_param_coeff = df_param_coeff.reset_index(drop=True)\n",
    "df_param_coeff.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "26710e3aa1a7bd1fd459322e8dc8ca68c2788051"
   },
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4ee6f018643d6c0e2511b9f1a6cccafa9cd68aeb"
   },
   "outputs": [],
   "source": [
    "## Confusion Matrix\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, rf_opt.predict(X_test))\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1994f9210c192db8d45221e68de2fed830b6b601"
   },
   "source": [
    "> The Confusion matrix provides us with a much more detailed representation of the accuracy score and of what's going on with our labels - we know exactly which/how labels were correctly and incorrectly predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1b65fd0f6e947c43df8337bd6a6eeaffc98db10f"
   },
   "outputs": [],
   "source": [
    "print('Accuracy of RandomForest Regression Classifier on test set: {:.2f}'.format(rf_opt.score(X_test, y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aa31c2cae746bef676288149ba8a4402eadff92b"
   },
   "outputs": [],
   "source": [
    "# Classification report for the optimised RF Regression\n",
    "rf_opt.fit(X_train, y_train)\n",
    "print(classification_report(y_test, rf_opt.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c74fef8c3244fcfecddb903318e73ecc72c1dd1d"
   },
   "source": [
    "> The resulting AUC score is higher than that best score during the optimisation step. Predicting probabilities of a particular label provides us with a measure of how likely an employee is to leave the company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3630c8d940858a4a2e279ad5e31661cd0738be62"
   },
   "outputs": [],
   "source": [
    "rf_opt.fit(X_train, y_train) # fit optimised model to the training data\n",
    "probs = rf_opt.predict_proba(X_test) # predict probabilities\n",
    "probs = probs[:, 1] # we will only keep probabilities associated with the employee leaving\n",
    "rf_opt_roc_auc = roc_auc_score(y_test, probs) # calculate AUC score using test dataset\n",
    "print('AUC score: %.3f' % rf_opt_roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1d96440f2b25b009a80faa1a6628cf1fb661610a"
   },
   "source": [
    "### ROC Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "352b0645744587ef925329e282c2b067b2aabfa2"
   },
   "source": [
    "> AUC - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. The green line represents the ROC curve of a purely random classifier; a good classifier stays as far away from that line as possible (toward the top-left corner). <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8817ed2d1a6efc5e274ef7ee34cb5683f16673c2"
   },
   "outputs": [],
   "source": [
    "# Create ROC Graph\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, log_opt.predict_proba(X_test)[:,1])\n",
    "rf_fpr, rf_tpr, rf_thresholds = roc_curve(y_test, rf_opt.predict_proba(X_test)[:,1])\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot Logistic Regression ROC\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "# Plot Random Forest ROC\n",
    "plt.plot(rf_fpr, rf_tpr, label='Random Forest (area = %0.2f)' % rf_opt_roc_auc)\n",
    "# Plot Base Rate ROC\n",
    "plt.plot([0,1], [0,1],label='Base Rate' 'k--')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Graph')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4496ca4685be2e26cc544361c90d21683a63b773"
   },
   "source": [
    "> As shown above, the fine-tuned Logistic Regression model showed a higher AUC score compared to the Random Forest Classifier. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7669ec2682823122cfef3e167d2316007bb25f9d"
   },
   "source": [
    "## Concluding Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0d35a39f71eb5adc96be4dc1613e12799fe59858"
   },
   "source": [
    "### Risk Category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2b8ef11f709e598764baaaae13ea5cc66b294051"
   },
   "source": [
    "As the company generates more data on its employees (on New Joiners and recent Leavers) the algorithm can be re-trained using the additional data and theoritically generate more accurate predictions to identify **high-risk employees** of leaving based on the probabilistic label assigned to each feature variable (i.e. employee) by the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2e6d84a350284b994486a53c2ddebce1c7edda73"
   },
   "source": [
    "Employees can be assigning a \"Risk Category\" based on the predicted label such that:\n",
    "- **Low-risk** for employees with label < 0.6\n",
    "- **Medium-risk** for employees with label between 0.6 and 0.8\n",
    "- **High-risk** for employees with label > 0.8 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d4f1cbfb803bde9c1e9e5ac133f11f01017acdeb"
   },
   "source": [
    "![title](https://images.unsplash.com/photo-1535017584024-2f4bead257df?ixlib=rb-1.2.1&auto=format&fit=crop&w=1350&q=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "de354cc3b56fe84fd61877e517ef364b41babb4f"
   },
   "source": [
    "### Strategic Retention Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2eb7f2b701d3a60f62790fe041d314aac51b6237"
   },
   "source": [
    "- The stronger indicators of people leaving include:\n",
    "    - **Monthly Income**: people on higher wages are less likely to leave the company. Hence, efforts should be made to gather information on industry benchmarks in the current local market to determine if the company is providing competitive wages.\n",
    "    - **Over Time**: people who work overtime are more likelty to leave the company. Hence efforts  must be taken to appropriately scope projects upfront with adequate support and manpower so as to reduce the use of overtime.\n",
    "    - **YearsWithCurrManager**: A large number of leavers leave 6 months after their Current Managers. By using Line Manager details for each employee, one can determine which Manager have experienced the largest numbers of employees resigning over the past year. Several metrics can be used here to determine whether action should be taken with a Line Manager: \n",
    "        - number of employees under managers showing high turnover rates: this would indicate that the organisation's structure may need to be revisit to improve efficiency\n",
    "        - number of years the Line Manager has been in a particular position: this may indicate that the employees may need management training or be assigned a mentor (ideally an Executive) in the organisation\n",
    "        - patterns in the employees who have resigned: this may indicate recurring patterns in employees leaving in which case action may be taken accordingly.\n",
    "    - **Age**: Employees in relatively young age bracket 25-35 are more likely to leave. Hence, efforts should be made to clearly articulate the long-term vision of the company and young employees fit in that vision, as well as provide incentives in the form of clear paths to promotion for instance.\n",
    "    - **DistanceFromHome**: Employees who live further from home are more likely to leave the company. Hence, efforts should be made to provide support in the form of company transportation for clusters of employees leaving the same area, or in the form of Transportation Allowance. Initial screening of employees based on their home location is probably not recommended as it would be regarded as a form of discrimination as long as employees make it to work on time every day.\n",
    "    - **TotalWorkingYears**: The more experienced employees are less likely to leave. Employees who have between 5-8 years of experience should be identified as potentially having a higher-risk of leaving.\n",
    "    - **YearsAtCompany**: Loyal companies are less likely to leave. Employees who hit their two-year anniversary should be identified as potentially having a higher-risk of leaving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dd6ad23b6d003fdbae64de623ac5fd89001841c9"
   },
   "source": [
    "A strategic **\"Retention Plan\"** should be drawn for each **Risk Category** group. In addition to the suggested steps for each feature listed above, face-to-face meetings between a HR representative and employees can be initiated for **medium-** and **high-risk employees** to discuss work conditions. Also, a meeting with those employee's Line Manager would allow to discuss the work environment within the team and whether steps can be taken to improve it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "798c2c4f8a353015f87e818843cde755072f4fee"
   },
   "source": [
    "* **I hope you enjoyed reading this Kernel as much as I had writing it. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d61db9367b2d4492006d2468d5f1b5baab02bf89"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "296px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
